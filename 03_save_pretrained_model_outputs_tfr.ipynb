{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99c8d58-92df-43a1-9008-be679b8a20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "import tensorboard\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.python.client import device_lib \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet101V2, Xception, InceptionResNetV2\n",
    "from tensorflow.keras.applications import resnet_v2, xception, inception_resnet_v2\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "global PROJECT_DIRECTORY\n",
    "PROJECT_DIRECTORY = os.getcwd()\n",
    "\n",
    "train_directory = \"./data/organized/train/\"\n",
    "val_directory = \"./data/organized/val/\"\n",
    "test_directory = \"./data/organized/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f915fbec-3be0-4af7-9c79-c81575e96875",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a87837-354b-4000-8f5a-13badee02d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_tf_dataset(tf_dataset):\n",
    "    for batch, labels in tf_dataset:\n",
    "        print(f\"Batch Shape: {batch.shape}\")\n",
    "        print(f\"Labels Shape: {labels.shape}\\n\")\n",
    "        print(f\"type Batch:\\n {batch.dtype}\\n\")\n",
    "        print(f\"type Labels:\\n {labels.dtype}\\n\")\n",
    "        print(\"\\n=======================================\")\n",
    "        print(f\"Feature Batch: {batch}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "        print(\"=======================================\\n\")\n",
    "        try:\n",
    "            print(f\"len(Batch):\\n {len(batch)}\\n\")\n",
    "            print(f\"len(Labels):\\n {len(labels)}\\n\")\n",
    "        except:\n",
    "            print(\"Can't call length on this!\")\n",
    "        \n",
    "        \n",
    "        for batch_num in range(1):\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            print(f\"Batch Item Number: {batch_num}\")\n",
    "            print(f\"Batch[{batch_num}].shape:\\n {batch[batch_num].shape}\\n\")\n",
    "            print(f\"Labels[{batch_num}].shape:\\n {labels[batch_num].shape}\\n\")\n",
    "            print(f\"type Batch[{batch_num}].shape:\\n {batch[batch_num].dtype}\\n\")\n",
    "            print(f\"type Labels[{batch_num}].shape:\\n {labels[batch_num].dtype}\\n\")\n",
    "            print(f\"Batch[{batch_num}]:\\n {batch[batch_num]}\\n\")\n",
    "            print(f\"Labels[{batch_num}]:\\n {labels[batch_num]}\\n\")\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb299c1-779c-4c28-b246-b14fc0afa49a",
   "metadata": {},
   "source": [
    "### Reading images from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedf219c-a413-44fd-af9b-21b8a4363d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorflow_datasets(image_size, train_directory, val_directory, test_directory, batch_size=32):\n",
    "    \n",
    "    train_dataset = image_dataset_from_directory(directory = train_directory,\n",
    "                                                 labels='inferred',\n",
    "                                                 label_mode = 'int',\n",
    "                                                 image_size=image_size,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 smart_resize=True)\n",
    "\n",
    "    val_dataset = image_dataset_from_directory(directory = val_directory,\n",
    "                                               labels='inferred',\n",
    "                                               label_mode = 'int',\n",
    "                                               image_size=image_size,\n",
    "                                               batch_size=batch_size,\n",
    "                                               smart_resize=True)\n",
    "\n",
    "    test_dataset = image_dataset_from_directory(directory = test_directory,\n",
    "                                                labels = \"inferred\",\n",
    "                                                label_mode = \"int\",\n",
    "                                                image_size=image_size,\n",
    "                                                batch_size=batch_size,\n",
    "                                                smart_resize=True)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b031eb9-1e2c-40d4-a7b7-41c5ea4f3016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10520 files belonging to 196 classes.\n",
      "Found 3234 files belonging to 196 classes.\n",
      "Found 2431 files belonging to 196 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = create_tensorflow_datasets(image_size=(520, 520),\n",
    "                                                                      train_directory=train_directory,\n",
    "                                                                      val_directory=val_directory,\n",
    "                                                                      test_directory=test_directory,\n",
    "                                                                      batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53981354-4fb9-4dd0-9068-c22eb6dd2db5",
   "metadata": {},
   "source": [
    "### Calculating the output of a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88381f58-e9d3-4421-9578-1841f3099d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_model_outputs(tf_dataset, base_model_type, input_shape, output_pooling):\n",
    "    \n",
    "    valid_base_model_types = ['resnet101', 'xception', 'inception_resnet']\n",
    "    \n",
    "    if base_model_type not in valid_base_model_types:\n",
    "        print(\"/n===========================================================\")\n",
    "        print(\"Invalid input for parameter base_model_type\")\n",
    "        print(f\"Valid inputs are: {valid_base_model_types}\")\n",
    "        print(\"===========================================================\\n\")\n",
    "        return -1\n",
    "    \n",
    "    target_labels = []\n",
    "    model_output_features = []\n",
    "    \n",
    "\n",
    "    data_augmentation = keras.Sequential([layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "                                          layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "                                          layers.experimental.preprocessing.RandomZoom(0.2)])\n",
    "    \n",
    "    \n",
    "    if base_model_type == 'resnet101':\n",
    "        \n",
    "        base_model =  ResNet101V2(weights='imagenet',\n",
    "                                  input_shape = input_shape,\n",
    "                                  include_top=False,\n",
    "                                  pooling = output_pooling)\n",
    "            \n",
    "        for images, labels in tf_dataset:\n",
    "                \n",
    "            augmented_images = data_augmentation(images)\n",
    "\n",
    "            preprocessed_images = resnet_v2.preprocess_input(augmented_images)\n",
    "\n",
    "            features = base_model.predict(preprocessed_images)\n",
    "\n",
    "            model_output_features.append(features)\n",
    "\n",
    "            target_labels.append(labels)\n",
    "\n",
    "        return np.concatenate(model_output_features), np.concatenate(target_labels)\n",
    "        \n",
    "    elif base_model_type == 'xception':\n",
    "        \n",
    "        base_model = Xception(weights='imagenet',\n",
    "                              input_shape=input_shape,\n",
    "                              include_top=False,\n",
    "                              pooling = output_pooling)\n",
    "        \n",
    "        for images, labels in tf_dataset:\n",
    "            \n",
    "            augmented_images = data_augmentation(images)\n",
    "            \n",
    "            preprocessed_images = xception.preprocess_input(augmented_images)\n",
    "            \n",
    "            features = base_model.predict(preprocessed_images)\n",
    "            \n",
    "            model_output_features.append(features)\n",
    "            \n",
    "            target_labels.append(labels)\n",
    "            \n",
    "        return np.concatenate(model_output_features), np.concatenate(target_labels)\n",
    "    \n",
    "    elif base_model_type == 'inception_resnet':\n",
    "        \n",
    "        base_model = InceptionResNetV2(weights='imagenet',\n",
    "                                       input_shape=input_shape,\n",
    "                                       include_top=False,\n",
    "                                       pooling = output_pooling)\n",
    "        \n",
    "        for images, labels in tf_dataset:\n",
    "            \n",
    "            augmented_images = data_augmentation(images)\n",
    "            \n",
    "            preprocessed_images = inception_resnet_v2.preprocess_input(augmented_images)\n",
    "            \n",
    "            features = base_model.predict(preprocessed_images)\n",
    "            \n",
    "            model_output_features.append(features)\n",
    "            \n",
    "            target_labels.append(labels)\n",
    "            \n",
    "        return (np.concatenate(model_output_features), np.concatenate(target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3967e66-5e21-4748-be3a-24e644189e0f",
   "metadata": {},
   "source": [
    "### Functions for writing Numpy Arrays to TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f148b661-c1f7-4be3-a599-1aca1a53057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_array(array):\n",
    "    array = tf.io.serialize_tensor(array)\n",
    "    return array\n",
    "\n",
    "def parse_single_image(image, label):\n",
    "  \n",
    "    #define the dictionary -- the structure -- of our single example\n",
    "    data = {\n",
    "        'dim_1' : _int64_feature(image.shape[0]),\n",
    "        'dim_2' : _int64_feature(image.shape[1]),\n",
    "        'dim_3' : _int64_feature(image.shape[2]),\n",
    "        'raw_image' : _bytes_feature(serialize_array(image)),\n",
    "        'label' : _int64_feature(label)\n",
    "    }\n",
    "    \n",
    "    #create an Example, wrapping the single features\n",
    "    out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "\n",
    "    return out\n",
    "\n",
    "def get_filename(base_model_type, input_shape, output_pooling, num_unique_epochs, current_epoch, dataset_type):\n",
    "    \n",
    "    global PROJECT_DIRECTORY\n",
    "    \n",
    "    # Directory that will contain all outputs from base_model_type (as many data augmented variations as we make) that are\n",
    "    # preprocessed in the same way... (same starting image size and output pooling).\n",
    "    base_dir_name = f\"pretrained_model_output_features/{base_model_type}_pool{str(output_pooling)}_inShape_{str(input_shape)}\"\n",
    "    base_save_directory = os.path.join(PROJECT_DIRECTORY, base_dir_name)\n",
    "    \n",
    "    # Adding a unique folder inside the above directory that specifies if this is a train, val or test set\n",
    "    full_dir_path = os.path.join(base_save_directory, f\"{dataset_type}_unique_epoch_{current_epoch}_of_{num_unique_epochs}\")\n",
    "    \n",
    "    os.makedirs(full_dir_path, exist_ok = True)\n",
    "    \n",
    "    filename = os.path.join(full_dir_path, f\"{dataset_type}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def write_images_to_multiple_shards(images, labels, base_model_type, input_shape, output_pooling, num_unique_epochs, current_epoch,\n",
    "                                    dataset_type, max_files:int=10):\n",
    "    \n",
    "    filename = get_filename(base_model_type = base_model_type,\n",
    "                            input_shape = input_shape,\n",
    "                            output_pooling = output_pooling,\n",
    "                            num_unique_epochs = num_unique_epochs,\n",
    "                            current_epoch = current_epoch,\n",
    "                            dataset_type = dataset_type)\n",
    "\n",
    "    #determine the number of shards (single TFRecord files) we need:\n",
    "    splits = (len(images)//max_files) + 1 #determine how many tfr shards are needed\n",
    "    \n",
    "    if len(images) % max_files == 0:\n",
    "        splits-=1\n",
    "        \n",
    "    print(f\"\\nUsing {splits} shard(s) for {len(images)} files, with up to {max_files} samples per shard\")\n",
    "\n",
    "    file_count = 0\n",
    "    \n",
    "    for shard_file_num in range(splits):\n",
    "        \n",
    "        current_shard_name = filename + f\"_{shard_file_num + 1}_of_{splits}.tfrecords\"\n",
    "        \n",
    "        \n",
    "        options = tf.io.TFRecordOptions(compression_type=\"ZLIB\")\n",
    "        writer = tf.io.TFRecordWriter(current_shard_name, options=options)\n",
    "        #writer = tf.io.TFRecordWriter(current_shard_name, tf.io.TFRecordOptions(compression_type = \"GZIP\"))\n",
    "\n",
    "        current_shard_count = 0\n",
    "        \n",
    "        while current_shard_count < max_files: #as long as our shard is not full\n",
    "            \n",
    "            #get the index of the file that we want to parse now\n",
    "            index = (shard_file_num * max_files) + current_shard_count\n",
    "            \n",
    "            if index == len(images): #when we have consumed the whole data, preempt generation\n",
    "                break\n",
    "            \n",
    "            current_image = images[index]\n",
    "            current_label = labels[index]\n",
    "            \n",
    "            output = parse_single_image(image = current_image, label = current_label)\n",
    "            \n",
    "            writer.write(output.SerializeToString())\n",
    "            current_shard_count+=1\n",
    "            file_count += 1\n",
    "        \n",
    "    writer.close()\n",
    "    print(\"\\n=======================================================================================\")\n",
    "    print(f\"Wrote {file_count} elements to TFRecord\")\n",
    "    print(f\"Base file path: {filename}\")\n",
    "    print(\"=======================================================================================\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f50ef79-dbaa-4585-b5c0-95a8b9b2ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_dataset, base_model_type, input_shape, output_pooling\n",
    "\n",
    "def create_base_model_output_dirs(train_dataset, val_dataset, test_dataset, base_model_type, input_shape, output_pooling, num_unique_epochs,\n",
    "                                  verbose = True):\n",
    "    \n",
    "    global_clock = time.time()\n",
    "    \n",
    "    dataset_types = ['train', 'val', 'test']\n",
    "    datasets = [train_dataset, val_dataset, test_dataset]\n",
    "    \n",
    "    for epoch_num in range(1, num_unique_epochs + 1):\n",
    "    \n",
    "        for dataset, dset_type in zip(datasets, dataset_types):\n",
    "            \n",
    "            if verbose:\n",
    "                start_time = time.time()\n",
    "                print(\"\\n=======================================================================================\")\n",
    "                print(f\"Calculating pretrained {base_model_type} outputs for the {dset_type} dataset.\")\n",
    "                print(f\"Processing epoch set {epoch_num } of {num_unique_epochs}\")\n",
    "                print(f\"Output pooling setting: {output_pooling}\")\n",
    "                print(f\"Size of images before {base_model_type} preprocessing: {input_shape}\")\n",
    "                print(\"=======================================================================================\\n\")\n",
    "            \n",
    "            output_features, labels = calculate_base_model_outputs(tf_dataset = dataset,\n",
    "                                                                   base_model_type = base_model_type,\n",
    "                                                                   input_shape = input_shape,\n",
    "                                                                   output_pooling = output_pooling)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"\\n=======================================================================================\")\n",
    "                print(f\"Finished calcuating {base_model_type} outputs!\")\n",
    "                print(f\"{base_model_type} {dset_type} outputs shape: {output_features.shape}\")\n",
    "                print(f\"Time spent getting features: {time.time() - start_time} seconds.\")\n",
    "                print(\"Starting to save outputs to tfrRecord file...\")\n",
    "                save_start_time = time.time()\n",
    "                print(\"=======================================================================================\\n\")\n",
    "            \n",
    "            write_images_to_multiple_shards(images = output_features,\n",
    "                                            labels = labels,\n",
    "                                            base_model_type = base_model_type,\n",
    "                                            input_shape = input_shape,\n",
    "                                            output_pooling = output_pooling,\n",
    "                                            num_unique_epochs = num_unique_epochs,\n",
    "                                            current_epoch = epoch_num,\n",
    "                                            dataset_type = dset_type)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"\\n=======================================================================================\")\n",
    "                print(\"Finished saving to disk!\")\n",
    "                print(f\"Time spent saving: {time.time() - save_start_time} seconds.\")\n",
    "                print(\"=======================================================================================\\n\")\n",
    "            \n",
    "            del output_features\n",
    "            del labels\n",
    "                \n",
    "    if verbose:\n",
    "        print(\"\\n=======================================================================================\")\n",
    "        print(\"Finished getting preprocessed features for all datasets and epochs!\")\n",
    "        print(f\"Time spent: {round(((time.time() - global_clock) / 60), 2)} minutes.\")\n",
    "        print(\"=======================================================================================\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "585ca6ec-98ee-4036-80a5-b7625c2c9805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================================================\n",
      "Calculating pretrained resnet101 outputs for the train dataset.\n",
      "Processing epoch set 1 of 1\n",
      "Output pooling setting: None\n",
      "Size of images before resnet101 preprocessing: (520, 520, 3)\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished calcuating resnet101 outputs!\n",
      "resnet101 train outputs shape: (10520, 17, 17, 2048)\n",
      "Time spent getting features: 383.49495458602905 seconds.\n",
      "Starting to save outputs to tfrRecord file...\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "Using 1052 shard(s) for 10520 files, with up to 10 samples per shard\n",
      "\n",
      "=======================================================================================\n",
      "Wrote 10520 elements to TFRecord\n",
      "Base file path: C:\\Users\\Braden\\Desktop\\Data_Science\\04_General_Assembly\\05_Projects\\03_car\\pretrained_model_output_features/resnet101_poolNone_inShape_(520, 520, 3)\\train_unique_epoch_1_of_1\\train\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished saving to disk!\n",
      "Time spent saving: 350.1042950153351 seconds.\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Calculating pretrained resnet101 outputs for the val dataset.\n",
      "Processing epoch set 1 of 1\n",
      "Output pooling setting: None\n",
      "Size of images before resnet101 preprocessing: (520, 520, 3)\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished calcuating resnet101 outputs!\n",
      "resnet101 val outputs shape: (3234, 17, 17, 2048)\n",
      "Time spent getting features: 78.18204426765442 seconds.\n",
      "Starting to save outputs to tfrRecord file...\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "Using 324 shard(s) for 3234 files, with up to 10 samples per shard\n",
      "\n",
      "=======================================================================================\n",
      "Wrote 3234 elements to TFRecord\n",
      "Base file path: C:\\Users\\Braden\\Desktop\\Data_Science\\04_General_Assembly\\05_Projects\\03_car\\pretrained_model_output_features/resnet101_poolNone_inShape_(520, 520, 3)\\val_unique_epoch_1_of_1\\val\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished saving to disk!\n",
      "Time spent saving: 103.36997175216675 seconds.\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Calculating pretrained resnet101 outputs for the test dataset.\n",
      "Processing epoch set 1 of 1\n",
      "Output pooling setting: None\n",
      "Size of images before resnet101 preprocessing: (520, 520, 3)\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished calcuating resnet101 outputs!\n",
      "resnet101 test outputs shape: (2431, 17, 17, 2048)\n",
      "Time spent getting features: 58.59127640724182 seconds.\n",
      "Starting to save outputs to tfrRecord file...\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "Using 244 shard(s) for 2431 files, with up to 10 samples per shard\n",
      "\n",
      "=======================================================================================\n",
      "Wrote 2431 elements to TFRecord\n",
      "Base file path: C:\\Users\\Braden\\Desktop\\Data_Science\\04_General_Assembly\\05_Projects\\03_car\\pretrained_model_output_features/resnet101_poolNone_inShape_(520, 520, 3)\\test_unique_epoch_1_of_1\\test\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished saving to disk!\n",
      "Time spent saving: 78.50089907646179 seconds.\n",
      "=======================================================================================\n",
      "\n",
      "\n",
      "=======================================================================================\n",
      "Finished getting preprocessed features for all datasets and epochs!\n",
      "Time spent: 17.59 minutes.\n",
      "=======================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_base_model_output_dirs(train_dataset = train_dataset,\n",
    "                              val_dataset = val_dataset,\n",
    "                              test_dataset = test_dataset,\n",
    "                              base_model_type = 'resnet101',\n",
    "                              input_shape = (520, 520, 3),\n",
    "                              output_pooling = 'None',\n",
    "                              num_unique_epochs = 1,\n",
    "                              verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116efe32-bea7-44f5-bf6f-73867d1ab3df",
   "metadata": {},
   "source": [
    "## Reading TFRecord Dataset back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef74a057-1e7e-4735-804a-2f3c505eb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tf_shard_paths(base_model_type, input_shape, output_pooling, epoch_num, dataset_type):\n",
    "    \n",
    "    global PROJECT_DIRECTORY\n",
    "    \n",
    "    valid_dataset_types = ['train', 'val', 'test']\n",
    "    \n",
    "    if dataset_type not in valid_dataset_types:\n",
    "        print(\"/n===========================================================\")\n",
    "        print(\"Invalid input for parameter dataset_type\")\n",
    "        print(f\"Valid inputs are: {valid_dataset_types}\")\n",
    "        print(\"===========================================================\\n\")\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    #base_dir_name = f\"pretrained_model_output_features/{base_model_type}_pool{str(output_pooling)}_inShape_{str(input_shape)}\"\n",
    "    base_outputs_directory = os.path.join(os.path.join(PROJECT_DIRECTORY, \"pretrained_model_output_features\"),\n",
    "                                          f\"{base_model_type}_pool{str(output_pooling)}_inShape_{str(input_shape)}\")\n",
    "    \n",
    "    \n",
    "    #base_outputs_directory = f\"./pretrained_model_output_features/{base_model_type}_pool{str(output_pooling)}_inShape_{str(input_shape)}/\"\n",
    "    all_dirs = os.listdir(base_outputs_directory)\n",
    "    \n",
    "    correct_dir = [directory for directory in all_dirs if (dataset_type in directory) and (f'epoch_{epoch_num}' in directory)]\n",
    "        \n",
    "    if len(correct_dir) != 1:\n",
    "        print(\"/n============================ Error ===============================\")\n",
    "        print(\"Invalid directory filtering!\")\n",
    "        print(f\"Filtering returned: {correct_dir}\")\n",
    "        return -1\n",
    "        print(\"=========================================================================\\n\")\n",
    "            \n",
    "    correct_dir = correct_dir[0]\n",
    "        \n",
    "    filepath = os.path.join(base_outputs_directory, correct_dir)\n",
    "    \n",
    "    files = os.listdir(filepath)\n",
    "    \n",
    "    full_file_paths = [os.path.join(filepath, file) for file in files]\n",
    "    \n",
    "    return full_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6036de3c-713c-4192-bfa4-ff914238a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecords(example):\n",
    "    \n",
    "    feature_description = {\n",
    "        \"dim_1\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dim_2\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"dim_3\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    \n",
    "    dim_1 = example['dim_1']\n",
    "    dim_2 = example['dim_2']\n",
    "    dim_3 = example['dim_3']\n",
    "    raw_image = example['raw_image']\n",
    "    label = example['label']\n",
    "    \n",
    "    feature = tf.io.parse_tensor(raw_image, out_type=tf.float32)\n",
    "    feature = tf.reshape(feature, shape=[dim_1, dim_2, dim_3])\n",
    "    \n",
    "    \n",
    "    return (feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e31f793-f2bc-459b-909a-ba876e2a18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_multiple_tfrecords(base_model_type, dataset_type, batch_size, input_shape, output_pooling, epoch_num, compression = \"ZLIB\"):\n",
    "    \n",
    "    filepaths = get_all_tf_shard_paths(base_model_type = base_model_type,\n",
    "                                     input_shape = input_shape,\n",
    "                                     output_pooling = output_pooling,\n",
    "                                     epoch_num = epoch_num,\n",
    "                                     dataset_type = dataset_type)\n",
    "    \n",
    "    print(f\"length of file paths: {len(filepaths)}\")\n",
    "    print(f\"first: {filepaths[0]}\")\n",
    "    print(f\"last: {filepaths[-1]}\")\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    #create the dataset\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, compression_type = compression,num_parallel_reads=AUTOTUNE)\n",
    "\n",
    "    #pass every single feature through our mapping function\n",
    "    dataset = dataset.map(parse_tfrecords).shuffle(batch_size * 10).batch(32).prefetch(AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6642ceb2-cc80-4be6-91bd-dc8fdd5fcd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of file paths: 1052\n",
      "first: C:\\Users\\Braden\\Desktop\\Data_Science\\04_General_Assembly\\05_Projects\\03_car\\pretrained_model_output_features\\resnet101_poolNone_inShape_(520, 520, 3)\\train_unique_epoch_1_of_1\\train_1000_of_1052.tfrecords\n",
      "last: C:\\Users\\Braden\\Desktop\\Data_Science\\04_General_Assembly\\05_Projects\\03_car\\pretrained_model_output_features\\resnet101_poolNone_inShape_(520, 520, 3)\\train_unique_epoch_1_of_1\\train_9_of_1052.tfrecords\n"
     ]
    }
   ],
   "source": [
    "train_big = get_dataset_from_multiple_tfrecords(base_model_type = 'resnet101',\n",
    "                                                dataset_type = 'train',\n",
    "                                                batch_size = 32,\n",
    "                                                input_shape = (520, 520, 3),\n",
    "                                                output_pooling = None,\n",
    "                                                epoch_num = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca054dea-c9e8-453d-95c0-38920324bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 17, 17, 2048)\n",
      "Labels Shape: (32,)\n",
      "\n",
      "type Batch:\n",
      " <dtype: 'float32'>\n",
      "\n",
      "type Labels:\n",
      " <dtype: 'int64'>\n",
      "\n",
      "\n",
      "=======================================\n",
      "Feature Batch: [[[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 2.6175022  0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.24441056 0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    1.3647491 ]\n",
      "   [0.         0.23657493 0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    2.0415845 ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.47952315]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    2.6054454 ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    2.814672  ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         7.232976   0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         3.4626439  0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.3514347  0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         2.9159496\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         5.1890535  0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         3.7138462  0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         3.0630946  0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         2.777239\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         6.683677   0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         2.2396593  0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         2.0877826  0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.01222289 0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.06091879 ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.04892514 ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         3.160055   ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         1.881828   0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         2.9747756  0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.6405518  0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         1.6485833\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]\n",
      "\n",
      "  [[0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]\n",
      "   [0.         0.         0.         ... 0.         0.\n",
      "    0.        ]]]]\n",
      "Labels: [ 22 129 167 141   0 189 131  90 139  69 175 129  59  17 125 187  54  44\n",
      "   5  38 117  71 162  81 180 132 108  65 159  39 117  31]\n",
      "=======================================\n",
      "\n",
      "len(Batch):\n",
      " 32\n",
      "\n",
      "len(Labels):\n",
      " 32\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Batch Item Number: 0\n",
      "Batch[0].shape:\n",
      " (17, 17, 2048)\n",
      "\n",
      "Labels[0].shape:\n",
      " ()\n",
      "\n",
      "type Batch[0].shape:\n",
      " <dtype: 'float32'>\n",
      "\n",
      "type Labels[0].shape:\n",
      " <dtype: 'int64'>\n",
      "\n",
      "Batch[0]:\n",
      " [[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 2.6175022  0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.24441056 0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "\n",
      "Labels[0]:\n",
      " 22\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "inspect_tf_dataset(train_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0805fe3-a128-49a2-b4ab-5e6ed8203680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f3c2052-7c8b-4e06-9758-27249dc8e280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Braden\\\\Desktop\\\\Data_Science\\\\04_General_Assembly\\\\05_Projects\\\\03_car'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd12f509-41cf-4194-8355-86391c220845",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_tf_dataset(base_model_type = \"resnet101\",\n",
    "                               dataset_type = \"train\",\n",
    "                               batch_size = 32,\n",
    "                               input_shape = (520, 520, 3),\n",
    "                               output_pooling = None,\n",
    "                               epoch_num = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdfad0b-e026-424f-9104-b5473e283746",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_dataset = get_tf_dataset(base_model_type = \"resnet101\",\n",
    "                               dataset_type = \"train\",\n",
    "                               batch_size = 32,\n",
    "                               input_shape = (520, 520, 3),\n",
    "                               output_pooling = None,\n",
    "                               epoch_num = 1)''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1ce4f-fa1e-48b6-8b4b-4b2e6374e978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7052ba18-288f-4915-9072-3bcd3f308396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import glob\n",
    "\n",
    "train_files = sorted(glob.glob('./train*.tfrecord'))\n",
    "for f_i, file in enumerate(train_files): \n",
    "    print(f_i) \n",
    "    total_images += sum([1 for _ in tf.python_io.tf_record_iterator(file)])''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5763b7c-32e5-415f-bd68-5573fea2128f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
